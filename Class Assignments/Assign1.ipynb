{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m load_mnist()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Train the neural network\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m results, W1, W2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# After training, print final results\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[4], line 85\u001b[0m, in \u001b[0;36mtrain_neural_network\u001b[1;34m(X_train, y_train, hidden_size, output_size, learning_rate, iterations)\u001b[0m\n\u001b[0;32m     83\u001b[0m E2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(E1, W2\u001b[38;5;241m.\u001b[39mT)  \u001b[38;5;66;03m# Backpropagated error\u001b[39;00m\n\u001b[0;32m     84\u001b[0m dZ1 \u001b[38;5;241m=\u001b[39m E2 \u001b[38;5;241m*\u001b[39m relu_derivative(Z1)  \u001b[38;5;66;03m# Derivative of error w.r.t. Z1 (for ReLU)\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m dW1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdZ1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m N  \u001b[38;5;66;03m# Gradient for W1\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[0;32m     88\u001b[0m W2 \u001b[38;5;241m=\u001b[39m W2 \u001b[38;5;241m-\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m dW2  \u001b[38;5;66;03m# Update weights for W2\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\satch\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\multiarray.py:741\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(a, b, out)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;124;03m    result_type(*arrays_and_dtypes)\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    736\u001b[0m \n\u001b[0;32m    737\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays_and_dtypes\n\u001b[1;32m--> 741\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mdot)\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot\u001b[39m(a, b, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    743\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;124;03m    dot(a, b, out=None)\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    829\u001b[0m \n\u001b[0;32m    830\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b, out)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivative of ReLU function\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Softmax activation function\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability improvement\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "# Categorical Cross-Entropy Loss function\n",
    "def categorical_crossentropy(predictions, labels):\n",
    "    return -np.mean(np.sum(labels * np.log(predictions + 1e-8), axis=1))\n",
    "\n",
    "# Accuracy function\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1)\n",
    "    return np.mean(correct)\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "def load_mnist():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Flatten images into 784-dimensional vectors\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) / 255.0  # Normalize to [0, 1]\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "\n",
    "    # One-hot encode the labels\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Initialize parameters\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.01  # Weights for the hidden layer\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.01  # Weights for the output layer\n",
    "    return W1, W2\n",
    "\n",
    "# Main training function\n",
    "def train_neural_network(X_train, y_train, hidden_size=128, output_size=10, learning_rate=0.01, iterations=10000):\n",
    "    input_size = X_train.shape[1]  # 784 input neurons (28x28 pixels)\n",
    "    N = X_train.shape[0]  # Number of training examples\n",
    "    \n",
    "    # Initialize weights\n",
    "    W1, W2 = initialize_weights(input_size, hidden_size, output_size)\n",
    "    \n",
    "    # DataFrame to store loss and accuracy\n",
    "    results = pd.DataFrame(columns=[\"loss\", \"accuracy\"])\n",
    "    \n",
    "    for itr in range(iterations):\n",
    "        # Feedforward propagation\n",
    "        Z1 = np.dot(X_train, W1)  # First layer pre-activation\n",
    "        A1 = relu(Z1)  # First layer activation (ReLU)\n",
    "        \n",
    "        Z2 = np.dot(A1, W2)  # Second layer pre-activation (output)\n",
    "        A2 = softmax(Z2)  # Second layer activation (output predictions using softmax)\n",
    "\n",
    "        # Calculate loss (Categorical Cross-Entropy)\n",
    "        loss = categorical_crossentropy(A2, y_train)\n",
    "        acc = accuracy(y_train, A2)\n",
    "        \n",
    "        # Store loss and accuracy for each iteration\n",
    "        new_row = pd.DataFrame({\"loss\": [loss], \"accuracy\": [acc]})\n",
    "        results = pd.concat([results, new_row], ignore_index=True)\n",
    "\n",
    "        # Backpropagation\n",
    "        # Gradient for softmax output layer\n",
    "        E1 = A2 - y_train  # Error at output layer\n",
    "        dW2 = np.dot(A1.T, E1) / N  # Gradient for W2\n",
    "        \n",
    "        # Error propagated to hidden layer\n",
    "        E2 = np.dot(E1, W2.T)  # Backpropagated error\n",
    "        dZ1 = E2 * relu_derivative(Z1)  # Derivative of error w.r.t. Z1 (for ReLU)\n",
    "        dW1 = np.dot(X_train.T, dZ1) / N  # Gradient for W1\n",
    "\n",
    "        # Update weights\n",
    "        W2 = W2 - learning_rate * dW2  # Update weights for W2\n",
    "        W1 = W1 - learning_rate * dW1  # Update weights for W1\n",
    "\n",
    "    return results, W1, W2\n",
    "\n",
    "# Load the dataset\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "\n",
    "# Train the neural network\n",
    "results, W1, W2 = train_neural_network(X_train, y_train)\n",
    "\n",
    "# After training, print final results\n",
    "print(\"Final loss:\", results['loss'].iloc[-1])\n",
    "print(\"Final accuracy:\", results['accuracy'].iloc[-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
