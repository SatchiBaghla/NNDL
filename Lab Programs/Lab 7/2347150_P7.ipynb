{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHNLT07AAgDV"
   },
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_IWdUr6PAjBz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUxm8MQ4DS-D"
   },
   "source": [
    "### 1. Dataset Preparation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev-LojaFBAQl"
   },
   "source": [
    "#### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "aXBa74q6BCLu",
    "outputId": "0c627a0c-42e3-4327-d01a-87b3c617626a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "      <th>Poet</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\r\\n                    Objects Used to Prop...</td>\n",
       "      <td>\\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, ...</td>\n",
       "      <td>Michelle Menting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\r\\n                    The New Church\\r\\r\\n...</td>\n",
       "      <td>\\r\\r\\nThe old cupola glinted above the clouds,...</td>\n",
       "      <td>Lucia Cherciu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\r\\r\\n                    Look for Me\\r\\r\\n   ...</td>\n",
       "      <td>\\r\\r\\nLook for me under the hood\\r\\r\\nof that ...</td>\n",
       "      <td>Ted Kooser</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\r\\r\\n                    Wild Life\\r\\r\\n     ...</td>\n",
       "      <td>\\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\n...</td>\n",
       "      <td>Grace Cavalieri</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\\r\\r\\n                    Umbrella\\r\\r\\n      ...</td>\n",
       "      <td>\\r\\r\\nWhen I push your button\\r\\r\\nyou fly off...</td>\n",
       "      <td>Connie Wanek</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Title  \\\n",
       "0           0  \\r\\r\\n                    Objects Used to Prop...   \n",
       "1           1  \\r\\r\\n                    The New Church\\r\\r\\n...   \n",
       "2           2  \\r\\r\\n                    Look for Me\\r\\r\\n   ...   \n",
       "3           3  \\r\\r\\n                    Wild Life\\r\\r\\n     ...   \n",
       "4           4  \\r\\r\\n                    Umbrella\\r\\r\\n      ...   \n",
       "\n",
       "                                                Poem              Poet Tags  \n",
       "0  \\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, ...  Michelle Menting  NaN  \n",
       "1  \\r\\r\\nThe old cupola glinted above the clouds,...     Lucia Cherciu  NaN  \n",
       "2  \\r\\r\\nLook for me under the hood\\r\\r\\nof that ...        Ted Kooser  NaN  \n",
       "3  \\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\n...   Grace Cavalieri  NaN  \n",
       "4  \\r\\r\\nWhen I push your button\\r\\r\\nyou fly off...      Connie Wanek  NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Dataset.csv\")\n",
    "dataset = dataset.drop_duplicates(subset=['Poem'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34-NcLatDho_"
   },
   "source": [
    "#### Dataset Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TG0d1W_lDjWo",
    "outputId": "7a270bab-8829-47d3-db1e-57594ee70816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 13754 entries, 0 to 13833\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  13754 non-null  int64 \n",
      " 1   Title       13754 non-null  object\n",
      " 2   Poem        13754 non-null  object\n",
      " 3   Poet        13754 non-null  object\n",
      " 4   Tags        12854 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 644.7+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JhEr9aJEGnB"
   },
   "source": [
    "#### Concatenate multiple poems into a single text corpus, separating them by newline characters for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "g1IYosOSEH4J"
   },
   "outputs": [],
   "source": [
    "df_cleaned = dataset.dropna(subset=['Tags'])\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['Poem'])\n",
    "\n",
    "corpus = \"\\n\".join(df_cleaned['Poem'].head(100).values)\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQI1cCF7FWN6"
   },
   "source": [
    "### 2. Data Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jj7HnbmFT9q"
   },
   "source": [
    "#### Convert the text to lowercase and remove special characters or punctuation if necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUVONYkFFfW-",
    "outputId": "ffbec91d-ee89-470f-949f-07bfe674fa2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invisible fish swim this ghost ocean now described by waves of sand, by water-worn rock. soon the fish will learn to walk. then humans will come ashore and paint dreams on the dying stone. then later, much later, the ocean floor will be punctuated by chevy trucks, carrying the dreamers decendants, who are going to the store.\n",
      "dont bother the earth spirit who lives here. she is working on a story. it is the oldest story in the world and it is delicate, changing. if she sees you watching she will invite you in for coffee, give you warm bread, and you will be obligated to stay and listen. but this is no ordinary story. you will have to endure earthquakes, lightning, the deaths of all those you love, the most blinding beauty. its a story so compelling you may never want to leave this is how she traps you. see that stone finger over there that is the only one who ever escaped.\n",
      "hour in which i consider hydrangea, a salt or sand plant, varietal, the question of varietals, the diet of every mother i know, pounds feels like , i have lost i have lost, yes, a sense of my own possible beauty, grown external, i externalize beauty. beauty occurs on the surface of plants the sun darkens the skin of my child, he is so small, he is beautiful (i can see it is obvious) and everything about him is beautiful. his hand swells from the bite spread of some insects venom because he is small. he appears to feel nothing. he smashes his skull against the floor. he screams. i hold him in my lap on the kitchen floor in front of an open freezer, pressing a pack of frozen clay against his forehead. he likes the cold. i see it is so obvious. hydrangea. when i move, when i walk pushing my childs stroller (it is both walking and pushing or hauling, sometimes, also, lifting it is having another body, an adjunct body composed of errand and weight and tenderness and no small amount of power), i imagine i can feel this small amount of weight, this pounds like , interfering with the twitch of every muscle in my body. as an object, a mother is confusing, a middle-aged mother with little spare flesh, i feel every inch of major muscle pulling against gravity and against the weight of my child, now sleeping. this is the hour for thinking hydrangea. let no man look at me. i stop to brush the drowsy childs little eye. his face. he barely considers his mother. i am all around him. why should he consider what is all around him perhaps what is missing is a subtle power of differentiation. i am in, therefore, a time of mass apprehensions.\n",
      "my fathers body is a map\n",
      "a record of his journey\n",
      "\n",
      "he carries a bullet\n",
      "lodged in his left thigh\n",
      "there is a hollow where it entered\n",
      "a protruding bump where it sleeps\n",
      "the doctors say it will never awaken\n",
      "\n",
      "it is the one souvenir he insists on keeping\n",
      "mother has her own opinionsb ca con inyour father is crazy\n",
      "\n",
      "as a child\n",
      "i wanted a scar just like my fathers\n",
      "bold and appalling a mushroom explosion\n",
      "that said i too was at war\n",
      "instead i settled for a grain of rice\n",
      "a scar so small look closely there\n",
      "here between the eyes\n",
      "a bit to the right\n",
      "there on the bridge of my nose\n",
      "\n",
      "father says i was too young to remember\n",
      "it happened while i was sleeping\n",
      "leaking roof the pounding rain\n",
      "drop after drop after drop\n",
      "\n",
      "it has long been forgotten this practice of the mother\n",
      "weaning a child she crushes the seeds of a green\n",
      "chili rubs it to her nipple what the child feels\n",
      "she too will share in this act of love\n",
      "my own mother says it was not meant\n",
      "to be cruel when cruelty she tells me\n",
      "is a childs lips torn from breast as proof\n",
      "back home the women wear teeth marks\n",
      "\n",
      "why are you still seventeen\n",
      "and drifting like a dog after dark,\n",
      "dragging a shadow youve found\n",
      "\n",
      "put it back where it belongs,\n",
      "and that bend of river, too. thats not the road\n",
      "you want, though you have it to yourself.\n",
      "\n",
      "gone are the cars that crawl to town\n",
      "from the reactors, a parade of insects, metallic,\n",
      "fuming along the one four-lane street.\n",
      "\n",
      "the poplars of the shelterbelt lean away\n",
      "from the bypass that never had much to pass by\n",
      "but coyote and rabbitbrush.\n",
      "\n",
      "pinpricks stabbed in a map too dark to read\n",
      "i stared at stars light-years away.\n",
      "listen. that hissing just a sprinkler\n",
      "\n",
      "damping down yesterday until its today.\n",
      "the cottonwoods shiver, or i do,\n",
      "every leaf rustling as if its the one\n",
      "\n",
      "about to tear itself, not i.\n",
      "memory takes the graveyard shift.\n",
      "\n",
      "yes, your childhood now a legend of fountains\n",
      " jorge gulln\n",
      "\n",
      "yes, your childhood, now a legend\n",
      "gone to weeds, still remembers the gray road\n",
      "that set out to cross the desert of the future.\n",
      "\n",
      "and how, always just ahead,\n",
      "gray water glittered, happy to be just a mirage.\n",
      "who steps off the gray bus at the depot\n",
      "\n",
      "sidewalks shudder all the way home.\n",
      "blinds close their scratchy eyes.\n",
      "who settles in your old room\n",
      "\n",
      "sniffy air sprawls as if it owns the place,\n",
      "and now your teenage secrets have no one to tell.\n",
      "for the spider laying claim to the corner,\n",
      "\n",
      "there is a stickiness to spin, that the living may beg\n",
      "to be wrapped in silk and devoured,\n",
      "leaving not even the flinch f\n",
      "============\n",
      "'invisible fish swim this ghost ocean now described by waves of sand, by water-worn rock. soon the fish will learn to walk. then humans will come ashore and paint dreams on the dying stone. then later, much later, the ocean floor will be punctuated by chevy trucks, carrying the dreamers decendants, who are going to the store.\\ndont bother the earth spirit who lives here. she is working on a story. it is the oldest story in the world and it is delicate, changing. if she sees you watching she will invite you in for coffee, give you warm bread, and you will be obligated to stay and listen. but this is no ordinary story. you will have to endure earthquakes, lightning, the deaths of all those you love, the most blinding beauty. its a story so compelling you may never want to leave this is how she traps you. see that stone finger over there that is the only one who ever escaped.\\nhour in which i consider hydrangea, a salt or sand plant, varietal, the question of varietals, the diet of every mother i know, pounds feels like , i have lost i have lost, yes, a sense of my own possible beauty, grown external, i externalize beauty. beauty occurs on the surface of plants the sun darkens the skin of my child, he is so small, he is beautiful (i can see it is obvious) and everything about him is beautiful. his hand swells from the bite spread of some insects venom because he is small. he appears to feel nothing. he smashes his skull against the floor. he screams. i hold him in my lap on the kitchen floor in front of an open freezer, pressing a pack of frozen clay against his forehead. he likes the cold. i see it is so obvious. hydrangea. when i move, when i walk pushing my childs stroller (it is both walking and pushing or hauling, sometimes, also, lifting it is having another body, an adjunct body composed of errand and weight and tenderness and no small amount of power), i imagine i can feel this small amount of weight, this pounds like , interfering with the twitch of every muscle in my body. as an object, a mother is confusing, a middle-aged mother with little spare flesh, i feel every inch of major muscle pulling against gravity and against the weight of my child, now sleeping. this is the hour for thinking hydrangea. let no man look at me. i stop to brush the drowsy childs little eye. his face. he barely considers his mother. i am all around him. why should he consider what is all around him perhaps what is missing is a subtle power of differentiation. i am in, therefore, a time of mass apprehensions.\\nmy fathers body is a map\\na record of his journey\\n\\nhe carries a bullet\\nlodged in his left thigh\\nthere is a hollow where it entered\\na protruding bump where it sleeps\\nthe doctors say it will never awaken\\n\\nit is the one souvenir he insists on keeping\\nmother has her own opinionsb ca con inyour father is crazy\\n\\nas a child\\ni wanted a scar just like my fathers\\nbold and appalling a mushroom explosion\\nthat said i too was at war\\ninstead i settled for a grain of rice\\na scar so small look closely there\\nhere between the eyes\\na bit to the right\\nthere on the bridge of my nose\\n\\nfather says i was too young to remember\\nit happened while i was sleeping\\nleaking roof the pounding rain\\ndrop after drop after drop\\n\\nit has long been forgotten this practice of the mother\\nweaning a child she crushes the seeds of a green\\nchili rubs it to her nipple what the child feels\\nshe too will share in this act of love\\nmy own mother says it was not meant\\nto be cruel when cruelty she tells me\\nis a childs lips torn from breast as proof\\nback home the women wear teeth marks\\n\\nwhy are you still seventeen\\nand drifting like a dog after dark,\\ndragging a shadow youve found\\n\\nput it back where it belongs,\\nand that bend of river, too. thats not the road\\nyou want, though you have it to yourself.\\n\\ngone are the cars that crawl to town\\nfrom the reactors, a parade of insects, metallic,\\nfuming along the one four-lane street.\\n\\nthe poplars of the shelterbelt lean away\\nfrom the bypass that never had much to pass by\\nbut coyote and rabbitbrush.\\n\\npinpricks stabbed in a map too dark to read\\ni stared at stars light-years away.\\nlisten. that hissing just a sprinkler\\n\\ndamping down yesterday until its today.\\nthe cottonwoods shiver, or i do,\\nevery leaf rustling as if its the one\\n\\nabout to tear itself, not i.\\nmemory takes the graveyard shift.\\n\\nyes, your childhood now a legend of fountains\\n jorge gulln\\n\\nyes, your childhood, now a legend\\ngone to weeds, still remembers the gray road\\nthat set out to cross the desert of the future.\\n\\nand how, always just ahead,\\ngray water glittered, happy to be just a mirage.\\nwho steps off the gray bus at the depot\\n\\nsidewalks shudder all the way home.\\nblinds close their scratchy eyes.\\nwho settles in your old room\\n\\nsniffy air sprawls as if it owns the place,\\nand now your teenage secrets have no one to tell.\\nfor the spider laying claim to the corner,\\n\\nthere is a stickiness to spin, that the living may beg\\nto be wrapped in silk and devoured,\\nleaving not even the flinch f'\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = corpus.lower()\n",
    "\n",
    "cleaned_text = re.sub(r'[^a-zA-Z\\s,\\'\"-.]', '', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\r\\r\\n\\n\\r\\r\\n', '\\n', cleaned_text)\n",
    "cleaned_text = re.sub(r'[\\r\\r]', '', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\n \\n', '\\n\\n', cleaned_text)\n",
    "\n",
    "cleaned_text = re.sub(r'\\n{2,}', '\\n\\n', cleaned_text)\n",
    "cleaned_text = re.sub(r' +', ' ', cleaned_text)\n",
    "cleaned_text = cleaned_text.strip()\n",
    "\n",
    "print(cleaned_text[:5000])\n",
    "print(\"============\")\n",
    "print(repr(cleaned_text[:5000]))\n",
    "\n",
    "corpus = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jbZDATXx7AY",
    "outputId": "02c381b4-bd74-40c8-e630-da365b8e5eae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 612\n",
      "Lines: 2790\n"
     ]
    }
   ],
   "source": [
    "paragraphs = corpus.split(\"\\n\\n\")\n",
    "\n",
    "# Now, split each paragraph into lines (split by single \\n)\n",
    "lines_in_paragraphs = [paragraph.split(\"\\n\") for paragraph in paragraphs]\n",
    "\n",
    "temp_lines_in_paragraphs = []\n",
    "\n",
    "lines = 0\n",
    "for para in lines_in_paragraphs:\n",
    "    temp = []\n",
    "    for line in para:\n",
    "      line = line.strip()\n",
    "      if line == \"\":\n",
    "        continue\n",
    "      temp.append(line)\n",
    "    if len(temp) > 0:\n",
    "      temp_lines_in_paragraphs.append(temp)\n",
    "\n",
    "    lines += len(para)\n",
    "\n",
    "print(f\"Paragraphs: {len(paragraphs)}\")\n",
    "print(f\"Lines: {lines}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBFlW6i6GfVA"
   },
   "source": [
    "#### Tokenize the text (e.g., convert each word to a unique integer).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgYXrEhhGegk",
    "outputId": "ba9a501d-440b-4a3a-99c5-312314fb48d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4685\n",
      "[[528, 370, 529, 24, 701, 163, 62, 1013, 36, 530, 3, 371, 36, 121, 1014, 702, 703, 1, 370, 25, 704, 4, 222, 82, 705, 25, 122, 1702, 6, 531, 284, 12, 1, 317, 706, 82, 223, 176, 223, 1, 163, 199, 25, 32, 1015, 36, 1703, 1016, 1704, 1, 1705, 1706, 47, 50, 106, 4, 1, 532], [107, 1707, 1, 135, 1708, 47, 247, 98, 33, 10, 1017, 12, 2, 177, 11, 10, 1, 1709, 177, 5, 1, 99, 6, 11, 10, 1710, 1711, 45, 33, 1712, 9, 533, 33, 25, 1713, 9, 5, 17, 318, 319, 9, 372, 1018, 6, 9, 25, 32, 1714, 4, 430, 6, 373, 30, 24, 10, 46, 1019, 177, 9, 25, 40, 4, 1715, 1716, 534, 1, 707, 3, 37, 224, 9, 123, 1, 374, 1717, 285, 35, 2, 177, 48, 1718, 9, 248, 83, 124, 4, 286, 24, 10, 53, 33, 1719, 9, 85, 13, 706, 708, 90, 56, 13, 10, 1, 94, 31, 47, 200, 1020], [535, 5, 125, 7, 1021, 709, 2, 710, 28, 371, 711, 1720, 1, 536, 3, 1721, 1, 1722, 3, 136, 80, 7, 100, 1022, 537, 14, 7, 40, 154, 7, 40, 154, 201, 2, 431, 3, 8, 137, 712, 285, 538, 1723, 7, 1724, 285, 285, 1023, 12, 1, 713, 3, 1725, 1, 249, 1726, 1, 202, 3, 8, 155, 22, 10, 48, 126, 22, 10, 225, 7, 64, 85, 11, 10, 1024, 6, 138, 78, 75, 10, 225, 16, 139, 1727, 34, 1, 1728, 1025, 3, 116, 1026, 1729, 86, 22, 10, 126, 22, 1027, 4, 149, 150, 22, 1730, 16, 1028, 140, 1, 199, 22, 1731, 7, 203, 75, 5, 8, 539, 12, 1, 540, 199, 5, 375, 3, 38, 320, 1732, 1733, 2, 1734, 3, 541, 1735, 140, 16, 432, 22, 1736, 1, 321, 7, 85, 11, 10, 48, 1024, 709, 49, 7, 1029, 49, 7, 222, 1030, 8, 542, 1737, 11, 10, 204, 543, 6, 1030, 28, 1738, 205, 544, 545, 11, 10, 546, 164, 58, 38, 1739, 58, 1740, 3, 1741, 6, 322, 6, 1742, 6, 46, 126, 1031, 3, 323, 7, 324, 7, 64, 149, 24, 126, 1031, 3, 322, 24, 1022, 14, 1743, 15, 1, 1744, 3, 136, 714, 5, 8, 58, 18, 38, 1032, 2, 80, 10, 1745, 2, 1033, 1746, 80, 15, 190, 1747, 715, 7, 149, 136, 1748, 3, 547, 714, 716, 140, 1749, 6, 140, 1, 322, 3, 8, 155, 62, 376, 24, 10, 1, 535, 17, 717, 709, 91, 46, 72, 141, 27, 29, 7, 226, 4, 1750, 1, 1034, 542, 190, 250, 16, 95, 22, 1035, 1751, 16, 80, 7, 101, 37, 108, 75, 206, 377, 22, 1021, 43, 10, 37, 108, 75, 548, 43, 10, 1036, 10, 2, 1752, 323, 3, 1753, 7, 101, 5, 718, 2, 76, 3, 433, 1754], [8, 434, 58, 10, 2, 178], [2, 1755, 3, 16, 719], [22, 1037, 2, 378], [1756, 5, 16, 179, 1757], [56, 10, 2, 1758, 51, 11, 1038], [2, 1759, 1760, 51, 11, 1039], [1, 1040, 70, 11, 25, 83, 1041], [11, 10, 1, 31, 1761, 22, 1762, 12, 720], [80, 65, 23, 137, 1763, 1764, 1765, 1766, 117, 10, 1767], [18, 2, 155], [7, 207, 2, 721, 102, 14, 8, 434], [1042, 6, 1768, 2, 1769, 1770], [13, 57, 7, 66, 19, 27, 379], [722, 7, 1043, 17, 2, 1771, 3, 723], [2, 721, 48, 126, 141, 1044, 56], [98, 227, 1, 129], [2, 724, 4, 1, 156]]\n"
     ]
    }
   ],
   "source": [
    "flattened_lines = [line for paragraph in temp_lines_in_paragraphs for line in paragraph]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(flattened_lines)\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "# Convert the corpus into a sequence of integers (tokens)\n",
    "sequences = tokenizer.texts_to_sequences(flattened_lines)\n",
    "print(sequences[:20])  # Print the first 20 tokenized words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAHEJNCiHC_s"
   },
   "source": [
    "#### Use a sliding window to create sequences of words for the LSTM model. For example, if n=5, create sequences of 5 words with the 6th word as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zsdAMmiTHCrQ",
    "outputId": "84c5162a-452b-468d-f9f0-43c60231b115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: 7098\n",
      "y shape: 7098\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 5  # Length of the sequence\n",
    "\n",
    "# Create input-output pairs (X, y) based on the sliding window\n",
    "X, y = [], []\n",
    "\n",
    "for line in sequences:\n",
    "    for i in range(sequence_length, len(line)):\n",
    "        X.append(line[i-sequence_length:i])\n",
    "        y.append(line[i])\n",
    "\n",
    "print(\"X shape:\", len(X))\n",
    "print(\"y shape:\", len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wn8KrhXyFkIV"
   },
   "source": [
    "#### Pad the sequences so that they all have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DppME3iPHqtL",
    "outputId": "730ae623-f3f4-49bf-90c2-7821338e2d84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded X shape: (7098, 5)\n"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(X, maxlen = sequence_length, padding='pre')\n",
    "y = np.array(y)\n",
    "print(\"Padded X shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SodivjFaH9i6"
   },
   "source": [
    "### 3. LSTM Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYyJjR3OIJr_"
   },
   "source": [
    "  o Define an LSTM model with the following structure:  \n",
    "    - An embedding layer with an appropriate input dimension (based on vocabulary size) and output dimension (e.g., 100).  \n",
    "    - One or two LSTM layers with 100 units each.  \n",
    "    - A dropout layer with a rate of 0.2 to prevent overfitting.  \n",
    "    - A dense output layer with softmax activation for word prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZHw_deWIKdu",
    "outputId": "68c78cdb-8645-44f1-eaa7-16520dc5df40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=X.shape[1]))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))  # Dropout layer to prevent overfitting\n",
    "model.add(Dense(vocab_size, activation='softmax'))  # Output layer for word prediction\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJW296QxJJuh"
   },
   "source": [
    "### 4. Training:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ISnvEbqJNZa"
   },
   "source": [
    "#### Compile the model with categorical cross-entropy as the loss function and accuracy as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsjT6noXJzG2",
    "outputId": "50ec4610-5c07-4a9d-fd31-33e696246bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape after one-hot encoding: (7098, 4685)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-hot encode the target labels\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "print(\"y shape after one-hot encoding:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPZjP6POJJmb"
   },
   "source": [
    "#### Train the model on the sequences for 10-20 epochs (or until it achieves satisfactory performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7aJOB686KAbm",
    "outputId": "7edc76cb-91b0-4765-d32d-8e793c99ebb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.0519 - loss: 7.9926\n",
      "Epoch 2/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0595 - loss: 6.7467\n",
      "Epoch 3/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.0595 - loss: 6.5361\n",
      "Epoch 4/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.0588 - loss: 6.4288\n",
      "Epoch 5/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.0645 - loss: 6.2862\n",
      "Epoch 6/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.0667 - loss: 6.2156\n",
      "Epoch 7/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.0685 - loss: 6.1313\n",
      "Epoch 8/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.0724 - loss: 5.9827\n",
      "Epoch 9/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0747 - loss: 5.9132\n",
      "Epoch 10/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.0790 - loss: 5.7368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x307299790>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0O3wtfm3u-W9"
   },
   "source": [
    "### 5. Text Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgE70ZE3OZQO",
    "outputId": "c23a7034-44c4-4389-9f8d-88d609cd5c68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon a time us is highway in a,\n",
      "by that marks who birch,\n",
      "and she protects of the,\n",
      "would in where like who,\n",
      "manila window like this you,\n",
      "was my proof of the,\n",
      "man around a face before,\n",
      "on this the maw fall,\n",
      "the milk of alone his,\n",
      "sake for not be sinkhole,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len, temperature=1.0, words_per_set=5):\n",
    "    previous_word = \"\"  # Track the previous word to avoid consecutive repetition\n",
    "    word_count = 0  # Track the number of words generated in the current set\n",
    "    for i in range(next_words):\n",
    "        # Tokenize the seed text\n",
    "        tokenized_text = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        # Pad the tokenized sequence\n",
    "        tokenized_text = pad_sequences([tokenized_text], maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "        # Predict the next word probabilities\n",
    "        predicted = model.predict(tokenized_text, verbose=0)[0]\n",
    "\n",
    "        # Apply temperature to predictions (to make them more diverse)\n",
    "        predicted = np.log(predicted + 1e-7) / temperature\n",
    "        predicted = np.exp(predicted) / np.sum(np.exp(predicted))  # Normalize to get valid probabilities\n",
    "\n",
    "        # Sample the next word based on the adjusted probabilities\n",
    "        predicted_word_idx = np.random.choice(len(predicted), p=predicted)\n",
    "        predicted_word = tokenizer.index_word.get(predicted_word_idx, '')\n",
    "\n",
    "        # Ensure the predicted word is not the same as the previous word\n",
    "        if predicted_word == previous_word:\n",
    "            continue  # Skip the word if it's the same as the previous one\n",
    "\n",
    "        if word_count != 0:\n",
    "          seed_text += \" \"\n",
    "\n",
    "        # Append the predicted word to the seed text\n",
    "        seed_text += predicted_word\n",
    "\n",
    "        # Update the previous word\n",
    "        previous_word = predicted_word\n",
    "\n",
    "        # Track the number of words in the current set\n",
    "        word_count += 1\n",
    "\n",
    "        # If we've reached the words_per_set limit, add a comma and reset word count\n",
    "        if word_count >= words_per_set:\n",
    "            seed_text += \",\\n\"\n",
    "            word_count = 0  # Reset the word count for the next set of words\n",
    "\n",
    "    return seed_text\n",
    "\n",
    "seed = \"once upon a time \"\n",
    "generated_poem = generate_text(seed, next_words=50, model=model, tokenizer=tokenizer, max_sequence_len=X.shape[1], temperature=0.7, words_per_set=5)\n",
    "print(generated_poem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDCWxlGXvf1K"
   },
   "source": [
    "### 6. Evaluation and Experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U2qpWhAcvg2E",
    "outputId": "207ec781-29d1-457e-951b-00c25df7cb51"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=X.shape[1]))\n",
    "model2.add(LSTM(50, return_sequences=True))\n",
    "model2.add(LSTM(50))\n",
    "model2.add(Dropout(0.4))  # Dropout layer to prevent overfitting\n",
    "model2.add(Dense(vocab_size, activation='softmax'))  # Output layer for word prediction\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tmwf95hOvoXK",
    "outputId": "bdea7a81-e375-4c8a-f767-16dc682c7f49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.0529 - loss: 8.1253\n",
      "Epoch 2/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0618 - loss: 6.7561\n",
      "Epoch 3/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0662 - loss: 6.6519\n",
      "Epoch 4/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0598 - loss: 6.5289\n",
      "Epoch 5/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0607 - loss: 6.4412\n",
      "Epoch 6/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0606 - loss: 6.3448\n",
      "Epoch 7/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.0574 - loss: 6.2794\n",
      "Epoch 8/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0625 - loss: 6.1859\n",
      "Epoch 9/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0644 - loss: 6.1563\n",
      "Epoch 10/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0670 - loss: 6.0989\n",
      "Epoch 11/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0649 - loss: 6.0502\n",
      "Epoch 12/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0668 - loss: 6.0027\n",
      "Epoch 13/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0652 - loss: 5.9379\n",
      "Epoch 14/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0703 - loss: 5.9027\n",
      "Epoch 15/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0712 - loss: 5.8160\n",
      "Epoch 16/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0704 - loss: 5.7620\n",
      "Epoch 17/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0792 - loss: 5.6594\n",
      "Epoch 18/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0805 - loss: 5.6123\n",
      "Epoch 19/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0717 - loss: 5.6193\n",
      "Epoch 20/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0806 - loss: 5.5493\n",
      "Epoch 21/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0717 - loss: 5.5324\n",
      "Epoch 22/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0776 - loss: 5.4670\n",
      "Epoch 23/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0858 - loss: 5.4107\n",
      "Epoch 24/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0865 - loss: 5.3944\n",
      "Epoch 25/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0847 - loss: 5.3095\n",
      "Epoch 26/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0865 - loss: 5.2872\n",
      "Epoch 27/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0895 - loss: 5.2146\n",
      "Epoch 28/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0886 - loss: 5.2148\n",
      "Epoch 29/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0961 - loss: 5.1198\n",
      "Epoch 30/30\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0940 - loss: 5.1202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x35e976bd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model2.fit(X, y, epochs=30, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IkJnkBPWv-Ue",
    "outputId": "21429eb7-bc8c-447c-dfbe-81477a4dcda7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am feeling then the neighbor in the do in,\n",
      "a draining members flowerstain means to face,\n",
      "the piece the drafted stoves visor away,\n",
      "and death medical manila the teeth one,\n",
      "across my despite bosses which out to,\n",
      "have will you the old here to,\n",
      "night will do blood like would thousand,\n",
      "cockpit\n"
     ]
    }
   ],
   "source": [
    "seed = \"I am feeling \"\n",
    "generated_poem = generate_text(seed, next_words=50, model=model2, tokenizer=tokenizer, max_sequence_len=X.shape[1], temperature=0.7, words_per_set=7)\n",
    "print(generated_poem)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
